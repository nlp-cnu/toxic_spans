{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from ast import literal_eval\n",
    "import itertools as it\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "import nltk.tokenize\n",
    "import tqdm\n",
    "import sklearn\n",
    "\n",
    "from system_evaluation import evaluation_technique as evaluate\n",
    "import convert_to_bert as ctb\n",
    "import span_find as sf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing (run before training)\n",
    "# Make sure you choose the right training data path\n",
    "# Goal of this cell is to reformat the data so it is a collection of tokens and tags, both grouped by their text id\n",
    "\n",
    "# Reading the dataset into a pandas dataframe\n",
    "initial_df = ctb.read_raw_data(train_path)\n",
    "# creating a new column that contains the tokenization of each text\n",
    "initial_df['tokenize'] = [ctb.tokenize_text(text) for text in initial_df['text']]\n",
    "# creating a new column that contains the tags for each token. Should be a 1-1 correspondence\n",
    "initial_df['tags'] = [ctb.tag_toxic_spans(text, initial_df['spans'][i]) for i, text in enumerate(initial_df['text'])]\n",
    "\n",
    "# path to write new dataframe to\n",
    "new_path = os.path.join('toxic_data', 'train_full.txt')\n",
    "tag_list = []\n",
    "\n",
    "# stores all tags in list\n",
    "for i, item in initial_df['tags'].iteritems():\n",
    "    for tag in item:\n",
    "        tag_list.append(tag)\n",
    "\n",
    "# creates the write dataframe and organizes the token columns simultaneously\n",
    "flatdata = pd.DataFrame([( index, value) for ( index, values) \n",
    "                         in initial_df[ 'tokenize' ].iteritems() for value in values], \n",
    "                             columns = [ 'index', 'tokens']).set_index( 'index' )\n",
    "# putting the tags into the dataframe\n",
    "flatdata['tags'] = tag_list\n",
    "# Indexing the dataframe by which post a token came from\n",
    "flatdata['Text #'] = ['Text: {}'.format(i + 1) for i in flatdata.index]\n",
    "# Verify the dataframe\n",
    "print(flatdata)\n",
    "# Write the dataframe to specified path\n",
    "flatdata.to_csv(new_path, sep='\\t', columns=['Text #', 'tokens', 'tags'], index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method to create the prediction mask for a given prediction\n",
    "# the input looks like (token_list, tag_list, seperator_list)\n",
    "def generate_pred_mask(token_tag_space_zip):\n",
    "    # initializes an index variable to count across the text\n",
    "    current_index = 0\n",
    "    # intitializes a prediction mask\n",
    "    pred_mask = []\n",
    "    \n",
    "    # Item 0 is the token, item 1 is the tag, item 2 is the seperator\n",
    "    for index, item in enumerate(token_tag_space_zip):\n",
    "        # check if item is tagged as toxic\n",
    "        if item[1] == 'Tox':\n",
    "            # target index is set to be the end of the current item\n",
    "            target = current_index + len(item[0])\n",
    "            # adds all indexes of the current toxic item to the span mask\n",
    "            while current_index < target:\n",
    "                pred_mask.append(current_index)\n",
    "                current_index += 1\n",
    "            # check if the next item is tagged as toxic and if there is a token after the current \n",
    "            if index + 1 < len(token_tag_space_zip) and token_tag_space_zip[index +1][1] == 'Tox':\n",
    "                # target index is set to the end of the seperation between current and next token\n",
    "                target = current_index + item[2]                \n",
    "                # adds all indexes of the current seperator to the span mask\n",
    "                while current_index < target:\n",
    "                    pred_mask.append(current_index)\n",
    "                    current_index += 1\n",
    "        # if the token isn't toxic, set the current index to the next token's index\n",
    "        else:\n",
    "            current_index += len(item[0]) + item[2]\n",
    "    return pred_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets the evaluation dataset\n",
    "eval_df = pd.read_csv('toxic_data/tsd_test.csv', header=0, keep_default_na=False)\n",
    "# gets tag predictions\n",
    "tags = pd.read_csv('tsd_eval_tags_trial_3.csv', sep='\\t', header=0)['tags']\n",
    "# cleans the prediction tags so they can be read as a list\n",
    "tags = [literal_eval(x.replace('\\n', '').replace(' ', ',')) for x in tags]\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i, tag in enumerate(tags):\n",
    "    # grabs the text from the evaluation dataset\n",
    "    post = eval_df['text'][i]\n",
    "    # creates a list of tokens from the dataset\n",
    "    tokens = post.split()\n",
    "    # grabs the seperators between the tokens\n",
    "    list_of_spaces = ctb.space_between_tokens(post, tokens)\n",
    "    # zips the tokens, tags, and seperators\n",
    "    token_tag_space = list(zip(tokens, tag, list_of_spaces + [0]))\n",
    "    # adds the prediction mask to the total list of predictions\n",
    "    predictions.append(generate_pred_mask(token_tag_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell after the previous\n",
    "# creates list of ids\n",
    "ids = np.arange(len(tags))\n",
    "# DO NOT CHANGE THIS CODE\n",
    "with open(\"spans-pred.txt\", \"w\") as out:\n",
    "    for uid, text_scores in zip(ids, predictions):\n",
    "        out.write(f\"{str(uid)}\\t{str(text_scores)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is to see what the prediction mask contains\n",
    "read_file = 'spans-pred.txt'\n",
    "eval_file = 'toxic_data/tsd_test.csv'\n",
    "write_file = 'extracted-t3.txt'\n",
    "\n",
    "span_df = pd.read_csv(read_file, sep='\\t', names=['index', 'span'], header=None, index_col=0)\n",
    "text_df = pd.read_csv(eval_file, header=0)\n",
    "\n",
    "toxic = [sf.extract_toxic_span(literal_eval(span_list), text) for span_list, text in zip(span_df['span'], text_df['text'])]\n",
    "toxic_df = pd.DataFrame({'text':text_df['text'], 'toxic':toxic})\n",
    "toxic_df.to_csv(write_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

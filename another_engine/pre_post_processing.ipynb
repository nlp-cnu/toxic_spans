{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from ast import literal_eval\n",
    "import itertools as it\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "import nltk.tokenize\n",
    "import tqdm\n",
    "import sklearn\n",
    "\n",
    "from system_evaluation import evaluation_technique as evaluate\n",
    "import convert_to_bert as ctb\n",
    "import span_find as sf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing (run before training)\n",
    "# Make sure you choose the right training data path\n",
    "# Goal of this cell is to reformat the data so it is a collection of tokens and tags, both grouped by their text id\n",
    "\n",
    "# Reading the dataset into a pandas dataframe\n",
    "initial_df = ctb.read_raw_data(train_path)\n",
    "# creating a new column that contains the tokenization of each text\n",
    "initial_df['tokenize'] = [ctb.tokenize_text(text) for text in initial_df['text']]\n",
    "# creating a new column that contains the tags for each token. Should be a 1-1 correspondence\n",
    "initial_df['tags'] = [ctb.tag_toxic_spans(text, initial_df['spans'][i]) for i, text in enumerate(initial_df['text'])]\n",
    "\n",
    "# path to write new dataframe to\n",
    "new_path = os.path.join('toxic_data', 'train_full.txt')\n",
    "tag_list = []\n",
    "\n",
    "# stores all tags in list\n",
    "for i, item in initial_df['tags'].iteritems():\n",
    "    for tag in item:\n",
    "        tag_list.append(tag)\n",
    "\n",
    "# creates the write dataframe and organizes the token columns simultaneously\n",
    "flatdata = pd.DataFrame([( index, value) for ( index, values) \n",
    "                         in initial_df[ 'tokenize' ].iteritems() for value in values], \n",
    "                             columns = [ 'index', 'tokens']).set_index( 'index' )\n",
    "# putting the tags into the dataframe\n",
    "flatdata['tags'] = tag_list\n",
    "# Indexing the dataframe by which post a token came from\n",
    "flatdata['Text #'] = ['Text: {}'.format(i + 1) for i in flatdata.index]\n",
    "# Verify the dataframe\n",
    "print(flatdata)\n",
    "# Write the dataframe to specified path\n",
    "flatdata.to_csv(new_path, sep='\\t', columns=['Text #', 'tokens', 'tags'], index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method to create the prediction mask for a given prediction\n",
    "# the input looks like (token_list, tag_list, seperator_list)\n",
    "def generate_pred_mask(token_tag_space_zip):\n",
    "    # initializes an index variable to count across the text\n",
    "    current_index = 0\n",
    "    # intitializes a prediction mask\n",
    "    pred_mask = []\n",
    "    \n",
    "    # Item 0 is the token, item 1 is the tag, item 2 is the seperator\n",
    "    for index, item in enumerate(token_tag_space_zip):\n",
    "        # check if item is tagged as toxic\n",
    "        if item[1] == 'Tox':\n",
    "            # target index is set to be the end of the current item\n",
    "            target = current_index + len(item[0]) - 1\n",
    "            # adds all indexes of the current toxic item to the span mask\n",
    "            while current_index <= target:\n",
    "                pred_mask.append(current_index)\n",
    "                current_index += 1\n",
    "            # check if the next item is tagged as toxic and if there is a token after the current \n",
    "            if index + 1 < len(token_tag_space_zip) and token_tag_space_zip[index +1][1] == 'Tox':\n",
    "                # target index is set to the end of the seperation between current and next token\n",
    "                target = current_index + item[2]                \n",
    "                # adds all indexes of the current seperator to the span mask\n",
    "                while current_index < target:\n",
    "                    pred_mask.append(current_index)\n",
    "                    current_index += 1\n",
    "            else:\n",
    "                current_index += item[2]\n",
    "        # if the token isn't toxic, set the current index to the next token's index\n",
    "        else:\n",
    "            current_index += len(item[0]) + item[2]\n",
    "    return pred_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntext_num = 100\\npost = eval_df['text'][text_num]\\nprint(post)\\n\\ntag = tags[text_num]\\ntokens = post.split()\\n# tokenized_sentence = transformers.BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True).encode(post)\\nprint(list(zip(tokens, tag)))\\nlist_of_spaces = ctb.space_between_tokens(post, tokens)\\nprint(list_of_spaces)\\ntoken_tag_space = list(zip(tokens, tag, list_of_spaces + [0]))\\nprint(token_tag_space)\\npred_mask = generate_pred_mask(token_tag_space)\\nprint(pred_mask)\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gets the evaluation dataset\n",
    "eval_df = pd.read_csv('toxic_data/tsd_test.csv', header=0, keep_default_na=False)\n",
    "# gets tag predictions\n",
    "tags = pd.read_csv('full_model_seq_128/tsd_eval_tags_seq_128.csv', sep='\\t', header=0)['tags']\n",
    "# cleans the prediction tags so they can be read as a list\n",
    "tags = [literal_eval(x.replace('\\n', '').replace(' ', ',')) for x in tags]\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i, tag in enumerate(tags):\n",
    "    # grabs the text from the evaluation dataset\n",
    "    post = eval_df['text'][i]\n",
    "    # creates a list of tokens from the dataset\n",
    "    tokens = post.split()\n",
    "    # grabs the seperators between the tokens\n",
    "    list_of_spaces = ctb.space_between_tokens(post, tokens)\n",
    "    # zips the tokens, tags, and seperators\n",
    "    token_tag_space = list(zip(tokens, tag, list_of_spaces + [0]))\n",
    "    # adds the prediction mask to the total list of predictions\n",
    "    predictions.append(generate_pred_mask(token_tag_space))\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "text_num = 100\n",
    "post = eval_df['text'][text_num]\n",
    "print(post)\n",
    "\n",
    "tag = tags[text_num]\n",
    "tokens = post.split()\n",
    "# tokenized_sentence = transformers.BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True).encode(post)\n",
    "print(list(zip(tokens, tag)))\n",
    "list_of_spaces = ctb.space_between_tokens(post, tokens)\n",
    "print(list_of_spaces)\n",
    "token_tag_space = list(zip(tokens, tag, list_of_spaces + [0]))\n",
    "print(token_tag_space)\n",
    "pred_mask = generate_pred_mask(token_tag_space)\n",
    "print(pred_mask)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell after the previous\n",
    "# creates list of ids\n",
    "ids = np.arange(len(tags))\n",
    "# DO NOT CHANGE THIS CODE\n",
    "with open(\"spans-pred.txt\", \"w\") as out:\n",
    "    for uid, text_scores in zip(ids, predictions):\n",
    "        out.write(f\"{str(uid)}\\t{str(text_scores)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is to see what the prediction mask contains\n",
    "read_file = 'spans-pred.txt'\n",
    "eval_file = 'toxic_data/tsd_test.csv'\n",
    "write_file = 'extracted-seq_128_fixed.txt'\n",
    "\n",
    "span_df = pd.read_csv(read_file, sep='\\t', names=['index', 'span'], header=None, index_col=0)\n",
    "text_df = pd.read_csv(eval_file, header=0)\n",
    "\n",
    "toxic = [sf.extract_toxic_span(literal_eval(span_list), text) for span_list, text in zip(span_df['span'], text_df['text'])]\n",
    "toxic_df = pd.DataFrame({'text':text_df['text'], 'toxic':toxic})\n",
    "toxic_df.to_csv(write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in dataset:                    357464\n",
      "Number of toxic tags in dataset:                 26715\n",
      "Number of non-toxic tags in dataset:            330749\n",
      "Ratio of toxic tags to whole dataset           0.0747\n",
      "Ratio of non-toxic tags to whole dataset:      0.9253\n"
     ]
    }
   ],
   "source": [
    "stats_df = pd.read_csv('toxic_data/full_train.csv', header=0, keep_default_na=False)\n",
    "stats_df['spans'] = stats_df.spans.apply(literal_eval)\n",
    "stats_df['tags'] = [ctb.tag_toxic_spans(text, stats_df['spans'][i]) for i, text in enumerate(stats_df['text'])]\n",
    "# print(stats_df)\n",
    "num_tags = 0\n",
    "num_tox = 0\n",
    "num_o = 0\n",
    "for tox_arr in stats_df['tags']:\n",
    "    num_tags += len(tox_arr)\n",
    "    for tag in tox_arr:\n",
    "        if tag == 'Tox':\n",
    "            num_tox += 1\n",
    "        elif tag == 'O':\n",
    "            num_o += 1\n",
    "        else:\n",
    "            print(\"ERROR\")\n",
    "\n",
    "print('Number of tokens in dataset:', '{:>25}'.format(num_tags))\n",
    "print('Number of toxic tags in dataset:', '{:>21}'.format(num_tox))\n",
    "print('Number of non-toxic tags in dataset:', '{:>17}'.format(num_o))\n",
    "\n",
    "ratio_o = num_o/num_tags\n",
    "ratio_tox = num_tox/num_tags\n",
    "\n",
    "print('Ratio of toxic tags to whole dataset', '{:>16.4f}'.format(ratio_tox))\n",
    "print('Ratio of non-toxic tags to whole dataset:', '{:>11.4f}'.format(ratio_o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
